{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have a model and data. How do we fit that model to the data?? AKA parameter estimation, inverse problems, training, etc.\n",
    "\n",
    "## Shooting Method for Parameter Fitting\n",
    "\n",
    "We have some model $u=f(p)$ where $p$ represents our parameters. We put in some parameters, $p$ and recieve our simulated data $u$. How should you choose $p$ such that u best fits that data? \n",
    "\n",
    "The **shooting method** directly uses this model by putting a cost function on the output $C(p)$. A common loss function is the L2-loss function:\n",
    "$$\n",
    "C(p)=\\|f(p)-y\\|\n",
    "$$\n",
    "where $C(p): \\mathbb{R}^{n} \\rightarrow R $ is a function that returns a scalar. The shooting method directly optimizes this cost function by having the optimizer generate data given new choices of p.\n",
    "\n",
    "Julia has several nonlinear optimization methods. Here's three: JuMP.jl, Optim.jl, NLopt.jl.\n",
    "\n",
    "In general there are two methods:\n",
    "\n",
    "Local Optimization - Attempt to find the best nearby extrema by finding a point where $\\frac{d C}{d p}=0$. \n",
    "\n",
    "Global Optimization - Attempt to explore teh whole space and find the best of the extrema. Global methods are extremley computaionally difficult. \n",
    "\n",
    "We'll look at local optimization.\n",
    "\n",
    "The simplist of this is gradient descent, where given the parameters $p_i$, the next step, $p_{i+1}$ is given by:\n",
    "$$\n",
    "p_{i+1}=p_{i}-\\alpha \\frac{d C}{d P}\n",
    "$$\n",
    "\n",
    "We could do this, in a first order fashion, slowing following the path of steepest descent. Or we could think of this problem using Newton's method... What if we just treat it like a rootfinding problem? Solving for when $\\frac{dC}{dP}=0$.\n",
    "\n",
    "Newton's method would then look like:\n",
    "\n",
    "\n",
    "$$\n",
    "p_{i+1}=p_{i}- \\frac{d C}{d p}\\left(\\frac{d}{d p} \\frac{d C}{d p}\\right)^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{i+1}=p_{i}-\\frac{d C}{d p}\\left( \\frac{d^2 C}{d p^2}\\right)^{-1} \n",
    "$$\n",
    "\n",
    "\n",
    "*(Reminder): Newton's Method is formally:*\n",
    "$$x_{n+1}=x_{n}-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)}$$\n",
    "\n",
    "\n",
    "But, $\\left( \\frac{d^2 C}{d p^2}\\right)$ is the hessian! (Jacobian of gradient). Thus we can rewrite our new problem to be:\n",
    "$$\n",
    "p_{i+1}=p_{i}-H\\left(p_{i}\\right)^{-1} \\frac{d C\\left(p_{i}\\right)}{d p}\n",
    "$$\n",
    "where $ H(p) $ is the Hessian matrix $H_{i j}=\\frac{d C}{d x_{i} d x_{j}}$\n",
    "\n",
    "Solving equations with a hessian is difficult. Many techniques attempt to avoid the Hessian (as wellas the jacobian). A common technique is BFGS which is a gradient-based optimization method that attempts to approximate the Hessian along the way to modify its stepping behavior. It uses the history of previously calculated points to build this hessian approximate. If you keep a constant lenght of history, you get the I-BFGS technique, which is one of the most common large-scale optimization techniques.\n",
    "\n",
    "# Connecting optimization and differential equations...\n",
    "Lets say we want to follow the gradient of the solution towards a local minimum. That would mean that the flow that we would wish to follow is given by an ODE... \n",
    "\n",
    "Specifically the ODE:\n",
    "$$\n",
    "p^{\\prime}=-\\frac{d C}{d p}\n",
    "$$\n",
    "\n",
    "Ok, we can apply Euler method to this and get:\n",
    "$$\n",
    "p_{n+1}=p_{n}-\\alpha \\frac{d C\\left(p_{n}\\right)}{d p}\n",
    "$$\n",
    "(We get gradient descent ^^). \n",
    "\n",
    "Now lets apply implicit Euler, we get:\n",
    "$$\n",
    "p_{n+1}=p_{n}-\\alpha \\frac{d C\\left(p_{n+1}\\right)}{d p}\n",
    "$$\n",
    "Which we can solve for zeros of $p_{n+1}$ as before with Newton's method.  \n",
    "\n",
    "\n",
    "# Neural Network Training as a shooting method for functions\n",
    "\n",
    "A one layer dense neuron is traditionally written as the function:\n",
    "$$\\text{layer}(x) = \\sigma .(Wx+b)$$\n",
    "\n",
    "<span style=\"color:red\"> A \"layer\" is a weight matrix $W$ dotted with the input array (the previous layer values) $x$ plus the bias array of each node. This equals an array of the next layer's values. After this has been computed, there is an activation function applied, $\\sigma$. I'm currently working on a \"neural nets from scatch\" implementation that could be linked here that explains this a little bit better. </span>\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\\text { where } x \\in \\mathbb{R}^{n}, W \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^{m} \\text { and } \\sigma \\text { is some choice of } \\mathbb{R} \\rightarrow \\mathbb{R} \\\\ \\text { nonlinear function, where the } . \\text { is the Julia dot to signify element-wise } \\\\ \\text { operation. }\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A traditional neural network, is a 3 neuron function...\n",
    "$$\n",
    "N N(x)=W_{3} \\sigma_{2} \\cdot\\left(W_{2} \\sigma_{1} \\cdot\\left(W_{1} x+b_{1}\\right)+b_{2}\\right)+b_{3}\n",
    "$$\n",
    "where the first layer is the input layer, second is the hidden layer, and the final is called the output layer. \n",
    "\n",
    "However, why do we need to restrict ourselves to only 1 hidden layer. This is where *deep neural networks (DNN)* come in. \n",
    "\n",
    "$$\n",
    "\\begin{array}{c}v_{i+1}=\\sigma_{i} \\cdot\\left(W_{i} v_{i}+b_{i}\\right) \\\\ v_{1}=x \\\\ D N N(x)=v_{n}\\end{array}\n",
    "$$\n",
    "\n",
    "for n layers. This theory gives a direct way to transform the fitting of an arbitrary function into a parameter shooting problem. Given an unknown function $f$, one can define a cost function:\n",
    "\n",
    "$$\n",
    "C(p)=\\|D N N(x ; p)-f(x)\\|\n",
    "$$\n",
    "(We are in discrete world though):\n",
    "$$\n",
    "C(p)=\\sum_{k}^{N}\\left\\|D N N\\left(x_{k} ; p\\right)-f\\left(x_{k}\\right)\\right\\|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> OK OK here's the cool part... </span>\n",
    "\n",
    "## Recurrent Neural Networks (RNN)\n",
    "\n",
    "A recurrent neural network is a network given by a recurrance relation:\n",
    "$$\n",
    "x_{k+1} = x_k + \\text{DNN}(x_k, k ;p)\n",
    "$$\n",
    "\n",
    "BUT this is the **exactly** equivalent to euler discretization with $\\Delta t = 1$, on the *neural ordinary differential equation* defined by: \n",
    "\n",
    "$$x' = \\text{DNN}(x,t;p)$$\n",
    "\n",
    "The neural net *is* the discrete ODE solver...\n",
    "\n",
    "# Computing Gradients\n",
    "Many problems training neural networks, to fitting differential equations, all have the same mathematical structure, which requires the ability to compute the gradient of a cost function given model evaluations. This reduces to computing the gradient of the model's output given the parameters. Proof:\n",
    "$$\n",
    "C(p)=\\sum_{i}^{N}\\left\\|f\\left(x_{i} ; p\\right)-y_{i}\\right\\|\n",
    "$$\n",
    "$$\n",
    "\\frac{d C}{d p}=\\sum_{i}^{N} 2\\left(f\\left(x_{i} ; p\\right)-y_{i}\\right) \\frac{d f\\left(x_{i}\\right)}{d p}\n",
    "$$\n",
    "How to efficiently compute $\\frac{df(x_i)}{dp}$ is the essential question for shooting-based parameter fitting.\n",
    "\n",
    "## Forward and Backwards Propogation and their Pros and Cons are covered in NeuralNetsFromScratch. (Not that anyone is actually reading this)...\n",
    "\n",
    "But ok after long maths, backward propogation wins! With one traverse of the graph we get all the derivatives of our output with respect to one input.\n",
    "\n",
    "**Sensitivity** The derivative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function L2_loss(σ,sol,W)\n",
    "    #σ is the function\n",
    "    #W is the weights\n",
    "    #Sol is the answer we're testing it off of\n",
    "    norm(σ(W) - sol)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
